{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow notebook for image classification\n",
    "\n",
    "\n",
    "Resize to 64x64 to reduce processing needed, can fix this later. \n",
    "\n",
    "Strat:\n",
    "\n",
    "Resize\n",
    "Conv layer with pooling? \n",
    "some drop out\n",
    "dense layer at the end with sigmoid output for classification\n",
    "will need label vec too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    files = [i for i in glob.glob(path, recursive=True)]\n",
    "    labels = np.array([0 if 'Picasso' in f else 1 for f in files])\n",
    "    _labels = tf.one_hot(labels, depth=2, on_value=1.0, off_value=0.0)\n",
    "    #print('labels!', labels)\n",
    "    return files, _labels\n",
    "\n",
    "\n",
    "#need labels as one hot vectors....somehow\n",
    "def batch_read_and_process(files, labels, batch_size, img_h=64, img_w=64):\n",
    "    images, label_index = tf.train.slice_input_producer([files, labels], shuffle=True)\n",
    "    reader = tf.WholeFileReader()\n",
    "    #_,img = reader.read(images) #get file contents\n",
    "    img = tf.read_file(images)\n",
    "    imgs = tf.image.decode_jpeg(img, channels=3) #given rgb jpegs\n",
    "    imgs_resized = tf.image.resize_images(imgs, [img_w, img_h])\n",
    "    print('images', imgs_resized.shape)\n",
    "    #_labels = tf.one_hot(labels, depth=2)\n",
    "\n",
    "    img_batch, label_batch = tf.train.batch([imgs_resized, label_index], batch_size=batch_size,\n",
    "                                           allow_smaller_final_batch=True)\n",
    "    return img_batch, label_batch\n",
    "    \n",
    "\n",
    "def model(images, training, drop_out=0.8, filters=32, kernel_size=[3,3], strides=1, activation=tf.nn.relu, padding='same'):\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        conv1_out = tf.layers.conv2d(images, filters, kernel_size, \n",
    "                                  strides=strides, activation=activation, padding=padding)\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        conv2_out = tf.layers.conv2d(conv1_out, filters, kernel_size, strides=strides,\n",
    "                                    activation=activation, padding=padding)\n",
    "    with tf.variable_scope(\"pool1\"):\n",
    "        pool1_out = tf.layers.max_pooling2d(conv2_out, pool_size=[2,2], strides=strides, padding='valid')\n",
    "        \n",
    "    with tf.variable_scope(\"dropout1\"):\n",
    "        drop1 = tf.layers.dropout(pool1_out, rate=drop_out, training=training)\n",
    "        \n",
    "    with tf.variable_scope(\"conv3\"):\n",
    "        conv3_out = tf.layers.conv2d(drop1, filters, kernel_size,\n",
    "                                  strides=strides, activation=activation, padding=padding)\n",
    "    with tf.variable_scope(\"conv4\"):\n",
    "        conv4_out = tf.layers.conv2d(conv3_out, filters, kernel_size,\n",
    "                                     strides=strides, activation=activation, padding=padding)\n",
    "    with tf.variable_scope(\"pool2\"):\n",
    "        pool2_out = tf.layers.max_pooling2d(conv4_out, pool_size=[2,2], strides=strides, padding='valid')\n",
    "    with tf.variable_scope(\"dropout2\"):\n",
    "        drop2 = tf.layers.dropout(pool2_out, rate=drop_out, training=training)\n",
    "        \n",
    "    #we need to flatten our dropout tensor before we pass it to the dense layer\n",
    "    with tf.variable_scope(\"dense\"):\n",
    "        drop_flat = tf.reshape(drop2, [-1, drop2.shape[1] * drop2.shape[2] * drop2.shape[3]])\n",
    "        dense = tf.layers.dense(inputs=drop_flat, units=2)\n",
    "    return dense\n",
    "        \n",
    "def loss(labels, logits):\n",
    "    #print(logits.get_shape()[3])\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    return loss\n",
    "    \n",
    "def accu(labels, logits):\n",
    "    corr_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(corr_pred, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def train(labels, logits, batch_size, n_epochs, learning_rate=1e-3, iters_per_epoch=None):\n",
    "    print(labels)\n",
    "    print(logits)\n",
    "    iter_per_epoch = iters_per_epoch\n",
    "    global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        _loss = loss(labels, logits)\n",
    "        accuracy = accu(labels, logits)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    print(_loss)\n",
    "    train_op = optimizer.minimize(_loss, global_step=global_step)\n",
    "    init = tf.global_variables_initializer()\n",
    "    epoch_dict = {}\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        try:\n",
    "            for n in range(n_epochs):\n",
    "                print('Starting epoch:  {}'.format(n))\n",
    "                data_dict = {'loss':[], 'accuracy':[]}\n",
    "                for _ in tqdm.trange(iter_per_epoch):\n",
    "                    if not (coord.should_stop()):\n",
    "                        o = sess.run([_loss, train_op, accuracy, global_step])\n",
    "                        #print(\"Loss: {:05f}, Accuracy: {:04f}, Global Step: {:04d}\".\\\n",
    "                        #    format(o[0], o[2], int(o[3]))) \n",
    "                        #print(labels.eval())\n",
    "                        #print(logits.eval())\n",
    "                        #print(tf.nn.softmax(logits).eval())\n",
    "                        #print(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels).eval())\n",
    "                        data_dict['loss'].append(o[0])\n",
    "                        data_dict['accuracy'].append(o[2])\n",
    "                epoch_dict[n] = data_dict\n",
    "                epoch_loss = sum(data_dict['loss']) / len(data_dict['loss'])\n",
    "                epoch_accu = sum(data_dict['accuracy']) / len(data_dict['accuracy'])\n",
    "                print(\"Epoch: {}, Epoch Loss: {}, Epoch Accuracy: {}\".format(n, epoch_loss, epoch_accu))\n",
    "                \n",
    "\n",
    "        except:\n",
    "            #stage_stop.set()\n",
    "            coord.request_stop()\n",
    "            raise\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images (64, 64, 3)\n",
      "Tensor(\"batch:1\", shape=(?, 2), dtype=float32)\n",
      "Tensor(\"dense/dense/BiasAdd:0\", shape=(?, 2), dtype=float32)\n",
      "Tensor(\"loss/Mean:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 10/62 [00:14<01:13,  1.41s/it]"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "path = 'artist_dataset/**/*.jpg'\n",
    "\n",
    "_files, _labels = load_data(path)\n",
    "iters_per_epoch = len(_files) // batch_size\n",
    "images, labels = batch_read_and_process(_files, _labels, batch_size)\n",
    "cnn = model(images, training=True)\n",
    "train(labels, cnn, batch_size, n_epochs, iters_per_epoch=iters_per_epoch)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'artist_dataset/**/*.jpg'\n",
    "\n",
    "_files, _labels = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
