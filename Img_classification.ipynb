{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses tensorflow to implement a convolutional neural network (CNN) for image classification. A CNN was chosen due to good image processing capabilities (via convolution). The model architecture is simple. The main unit is two convolution layers, followed by a pooling and dropout layer. This unit is repeated twice, followed by a dense layer for the model output. This architecture was chosen as it (loosely) follows the inception architecture, which has historically had good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Build and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    files = [i for i in glob.glob(path, recursive=True)]\n",
    "    labels = np.array([0 if 'Picasso' in f else 1 for f in files])\n",
    "    _labels = tf.one_hot(labels, depth=2, on_value=1.0, off_value=0.0)\n",
    "    return files, _labels\n",
    "\n",
    "\n",
    "def batch_read_and_process(files, labels, batch_size, img_h=64, img_w=64):\n",
    "    images, label_index = tf.train.slice_input_producer([files, labels], shuffle=True)\n",
    "    reader = tf.WholeFileReader()\n",
    "    img = tf.read_file(images)\n",
    "    imgs = tf.image.decode_jpeg(img, channels=3) #given rgb jpegs\n",
    "    imgs_resized = tf.image.resize_images(imgs, [img_w, img_h])\n",
    "    img_batch, label_batch = tf.train.batch([imgs_resized, label_index], batch_size=batch_size,\n",
    "                                           allow_smaller_final_batch=True)\n",
    "    return img_batch, label_batch\n",
    "    \n",
    "\n",
    "def model(images, training, drop_out=0.8, filters=32, kernel_size=[3,3], strides=1, activation=tf.nn.relu, padding='same'):\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        conv1_out = tf.layers.conv2d(images, filters, kernel_size, \n",
    "                                  strides=strides, activation=activation, padding=padding)\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        conv2_out = tf.layers.conv2d(conv1_out, filters, kernel_size, strides=strides,\n",
    "                                    activation=activation, padding=padding)\n",
    "    with tf.variable_scope(\"pool1\"):\n",
    "        pool1_out = tf.layers.max_pooling2d(conv2_out, pool_size=[2,2], strides=strides, padding='valid')\n",
    "        \n",
    "    with tf.variable_scope(\"dropout1\"):\n",
    "        drop1 = tf.layers.dropout(pool1_out, rate=drop_out, training=training)\n",
    "        \n",
    "    with tf.variable_scope(\"conv3\"):\n",
    "        conv3_out = tf.layers.conv2d(drop1, filters, kernel_size,\n",
    "                                  strides=strides, activation=activation, padding=padding)\n",
    "    with tf.variable_scope(\"conv4\"):\n",
    "        conv4_out = tf.layers.conv2d(conv3_out, filters, kernel_size,\n",
    "                                     strides=strides, activation=activation, padding=padding)\n",
    "    with tf.variable_scope(\"pool2\"):\n",
    "        pool2_out = tf.layers.max_pooling2d(conv4_out, pool_size=[2,2], strides=strides, padding='valid')\n",
    "    with tf.variable_scope(\"dropout2\"):\n",
    "        drop2 = tf.layers.dropout(pool2_out, rate=drop_out, training=training)\n",
    "        \n",
    "    #we need to flatten our dropout tensor before we pass it to the dense layer\n",
    "    with tf.variable_scope(\"dense\"):\n",
    "        drop_flat = tf.reshape(drop2, [-1, drop2.shape[1] * drop2.shape[2] * drop2.shape[3]])\n",
    "        dense = tf.layers.dense(inputs=drop_flat, units=2)\n",
    "    return dense\n",
    "        \n",
    "def loss(labels, logits):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    return loss\n",
    "    \n",
    "def accu(labels, logits):\n",
    "    corr_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(corr_pred, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def train(labels, logits, batch_size, n_epochs, learning_rate=1e-3, iters_per_epoch=None):\n",
    "    iter_per_epoch = iters_per_epoch\n",
    "    global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        _loss = loss(labels, logits)\n",
    "        accuracy = accu(labels, logits)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(_loss, global_step=global_step)\n",
    "    init = tf.global_variables_initializer()\n",
    "    epoch_dict = {}\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        try:\n",
    "            for n in range(n_epochs):\n",
    "                print('Starting epoch:  {}'.format(n))\n",
    "                data_dict = {'loss':[], 'accuracy':[]}\n",
    "                for _ in tqdm.trange(iter_per_epoch):\n",
    "                    if not (coord.should_stop()):\n",
    "                        o = sess.run([_loss, train_op, accuracy, global_step])\n",
    "                        data_dict['loss'].append(o[0])\n",
    "                        data_dict['accuracy'].append(o[2])\n",
    "                epoch_dict[n] = data_dict\n",
    "                epoch_loss = sum(data_dict['loss']) / len(data_dict['loss'])\n",
    "                epoch_accu = sum(data_dict['accuracy']) / len(data_dict['accuracy'])\n",
    "                print(\"Epoch: {}, Epoch Loss: {}, Epoch Accuracy: {}\".format(n, epoch_loss, epoch_accu))\n",
    "                \n",
    "\n",
    "        except:\n",
    "            coord.request_stop()\n",
    "            raise\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:16<00:00,  1.04it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Epoch Loss: 111.60706950969333, Epoch Accuracy: 0.5458227810980398\n",
      "Starting epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:22<00:00,  1.04s/it]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Epoch Loss: 0.6670484980450401, Epoch Accuracy: 0.582784810775443\n",
      "Starting epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:16<00:00,  1.03it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Epoch Loss: 0.5984764065169081, Epoch Accuracy: 0.6754430407964731\n",
      "Starting epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:14<00:00,  1.06it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Epoch Loss: 0.5577416797227497, Epoch Accuracy: 0.7108860808082774\n",
      "Starting epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:18<00:00,  1.00it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Epoch Loss: 0.5425926228112812, Epoch Accuracy: 0.7255696235578272\n",
      "Starting epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:15<00:00,  1.04it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Epoch Loss: 0.5158670491810087, Epoch Accuracy: 0.7463291181793695\n",
      "Starting epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:12<00:00,  1.09it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Epoch Loss: 0.4687526939035971, Epoch Accuracy: 0.7777215185799177\n",
      "Starting epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:11<00:00,  1.10it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Epoch Loss: 0.463629593577566, Epoch Accuracy: 0.7827848113035853\n",
      "Starting epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:14<00:00,  1.06it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Epoch Loss: 0.4340533014339737, Epoch Accuracy: 0.8060759487031381\n",
      "Starting epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:20<00:00,  1.01s/it]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Epoch Loss: 0.44177883297582216, Epoch Accuracy: 0.7918987296804597\n",
      "Starting epoch:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:34<00:00,  1.19s/it]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Epoch Loss: 0.4074190460805652, Epoch Accuracy: 0.8086075971398172\n",
      "Starting epoch:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:32<00:00,  1.17s/it]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Epoch Loss: 0.3833834510060805, Epoch Accuracy: 0.8237974643707275\n",
      "Starting epoch:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:29<00:00,  1.14s/it]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Epoch Loss: 0.3902995280827148, Epoch Accuracy: 0.8222784799865529\n",
      "Starting epoch:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:32<00:00,  1.17s/it]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Epoch Loss: 0.3591153240656551, Epoch Accuracy: 0.841012655934201\n",
      "Starting epoch:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:40<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Epoch Loss: 0.34730357073153123, Epoch Accuracy: 0.8491139245938651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "batch_size = 25\n",
    "\n",
    "path = 'artist_dataset/**/*.jpg'\n",
    "\n",
    "_files, _labels = load_data(path)\n",
    "iters_per_epoch = len(_files) // batch_size\n",
    "images, labels = batch_read_and_process(_files, _labels, batch_size, img_h=64, img_w=64)\n",
    "cnn = model(images, training=True, drop_out=0.5) #dropout rate suggested by http://papers.nips.cc/paper/4878-understanding-dropout.pdf\n",
    "train(labels, cnn, batch_size, n_epochs, iters_per_epoch=iters_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code will optimize the filter (representing number of filters/features in convolutional layers) hyperparameter. This will act to make the model more or less complex, depending on the number of filters. As far as I'm aware, there isn't a straightforward functionality in TensorFlow to automatically tune hyperparameters, so the approach here will simply search across an array of possible values to find the optimal parameter, evaluated via epoch accuracy. Due to memory/cpu constraints on my machine, I will evaluate 8 parameter valeus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here I'm modifying the training function to not print out the results of every epoch, only the final epoch\n",
    "\n",
    "def opt_model(images, training, drop_out=0.8, filters=32, kernel_size=[3,3], strides=1, activation=tf.nn.relu, padding='same'):\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        conv1_out = tf.layers.conv2d(images, filters, kernel_size, \n",
    "                                  strides=strides, activation=activation, padding=padding)\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        conv2_out = tf.layers.conv2d(conv1_out, filters, kernel_size, strides=strides,\n",
    "                                    activation=activation, padding=padding)\n",
    "    with tf.variable_scope(\"pool1\"):\n",
    "        pool1_out = tf.layers.max_pooling2d(conv2_out, pool_size=[2,2], strides=strides, padding='valid')\n",
    "        \n",
    "    with tf.variable_scope(\"dropout1\"):\n",
    "        drop1 = tf.layers.dropout(pool1_out, rate=drop_out, training=training)\n",
    "        \n",
    "    with tf.variable_scope(\"conv3\"):\n",
    "        conv3_out = tf.layers.conv2d(drop1, filters, kernel_size,\n",
    "                                  strides=strides, activation=activation, padding=padding)\n",
    "    with tf.variable_scope(\"conv4\"):\n",
    "        conv4_out = tf.layers.conv2d(conv3_out, filters, kernel_size,\n",
    "                                     strides=strides, activation=activation, padding=padding)\n",
    "    with tf.variable_scope(\"pool2\"):\n",
    "        pool2_out = tf.layers.max_pooling2d(conv4_out, pool_size=[2,2], strides=strides, padding='valid')\n",
    "    with tf.variable_scope(\"dropout2\"):\n",
    "        drop2 = tf.layers.dropout(pool2_out, rate=drop_out, training=training)\n",
    "        \n",
    "    #we need to flatten our dropout tensor before we pass it to the dense layer\n",
    "    with tf.variable_scope(\"dense\"):\n",
    "        drop_flat = tf.reshape(drop2, [-1, drop2.shape[1] * drop2.shape[2] * drop2.shape[3]])\n",
    "        dense = tf.layers.dense(inputs=drop_flat, units=2)\n",
    "    return dense\n",
    "\n",
    "\n",
    "def hyperparameter_train(labels, logits, batch_size, n_epochs, learning_rate=1e-3, iters_per_epoch=None):\n",
    "    \n",
    "    iter_per_epoch = iters_per_epoch\n",
    "    global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        _loss = loss(labels, logits)\n",
    "        accuracy = accu(labels, logits)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(_loss, global_step=global_step)\n",
    "    init = tf.global_variables_initializer()\n",
    "    epoch_dict = {}\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        try:\n",
    "            for n in range(n_epochs):\n",
    "                #print('Starting epoch:  {}'.format(n))\n",
    "                data_dict = {'loss':[], 'accuracy':[]}\n",
    "                for _ in range(iter_per_epoch):\n",
    "                    if not (coord.should_stop()):\n",
    "                        o = sess.run([_loss, train_op, accuracy, global_step])\n",
    "                        data_dict['loss'].append(o[0])\n",
    "                        data_dict['accuracy'].append(o[2])\n",
    "                epoch_dict[n] = data_dict\n",
    "                epoch_loss = sum(data_dict['loss']) / len(data_dict['loss'])\n",
    "                epoch_accu = sum(data_dict['accuracy']) / len(data_dict['accuracy'])\n",
    "                #print(\"Epoch: {}, Epoch Loss: {}, Epoch Accuracy: {}\".format(n, epoch_loss, epoch_accu))\n",
    "                if n == (n_epochs - 1):\n",
    "                    final_loss = sum(epoch_dict[n]['loss']) / len(epoch_dict[n]['loss'])\n",
    "                    final_accu = sum(epoch_dict[n]['accuracy']) / len(epoch_dict[n]['accuracy'])\n",
    "                    print(\"Final Epoch Loss: {}, Final Epoch Accuracy: {}\".format(epoch_loss, epoch_accu)) \n",
    "                \n",
    "\n",
    "        except:\n",
    "            coord.request_stop()\n",
    "            raise\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filters:  1\n",
      "Final Epoch Loss: 0.5872002772138089, Final Epoch Accuracy: 0.6708860827397697\n",
      "Number of filters:  3\n",
      "Final Epoch Loss: 0.551719833401185, Final Epoch Accuracy: 0.7174683582933643\n",
      "Number of filters:  5\n",
      "Final Epoch Loss: 0.69078717126122, Final Epoch Accuracy: 0.5625316466711745\n",
      "Number of filters:  10\n",
      "Final Epoch Loss: 0.5898973515516595, Final Epoch Accuracy: 0.6739240571667876\n",
      "Number of filters:  15\n",
      "Final Epoch Loss: 0.5149389082872415, Final Epoch Accuracy: 0.7458227887938295\n",
      "Number of filters:  20\n",
      "Final Epoch Loss: 0.5424101382871217, Final Epoch Accuracy: 0.7265822793109508\n",
      "Number of filters:  32\n",
      "Final Epoch Loss: 0.37788925367065623, Final Epoch Accuracy: 0.832911387274537\n",
      "Number of filters:  64\n",
      "Final Epoch Loss: 0.4134841419850724, Final Epoch Accuracy: 0.8086075911039039\n"
     ]
    }
   ],
   "source": [
    "filter_array = [1,3,5,10,15,20,32,64] #filter values we're going to search through\n",
    "\n",
    "#we will wrap data and training ops in a for loop to run through filter values\n",
    "#using a smaller number of epochs\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 25\n",
    "\n",
    "path = 'artist_dataset/**/*.jpg'\n",
    "\n",
    "\n",
    "\n",
    "for i in filter_array:\n",
    "    print(\"Number of filters:  {}\".format(i))\n",
    "    tf.reset_default_graph()\n",
    "    _files, _labels = load_data(path)\n",
    "    iters_per_epoch = len(_files) // batch_size\n",
    "    images, labels = batch_read_and_process(_files, _labels, batch_size, img_h=64, img_w=64)\n",
    "    opt_cnn = opt_model(images, filters=i, training=True, drop_out=0.5) #dropout rate suggested by http://papers.nips.cc/paper/4878-understanding-dropout.pdf\n",
    "    hyperparameter_train(labels, opt_cnn, batch_size, n_epochs, iters_per_epoch=iters_per_epoch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, 3 filters would probably achieve a good accuracy, given a few more training epochs. This would also reduce the complexity of the default model (32 filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
